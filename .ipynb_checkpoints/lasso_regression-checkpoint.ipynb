{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "64688f4c",
   "metadata": {},
   "source": [
    "# Lasso Regression Implementation and Analysis\n",
    "\n",
    "This notebook demonstrates the implementation of Lasso Regression using coordinate descent optimization. It includes:\n",
    "- Synthetic data generation\n",
    "- Implementation of Lasso regression from scratch\n",
    "- Cross-validation for hyperparameter selection\n",
    "- Visualization of results\n",
    "\n",
    "Originally created by hilmi, adapted to Jupyter notebook format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ec84c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import KFold  # Only for cross-validation split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b654fd1d",
   "metadata": {},
   "source": [
    "## 1. Generate Synthetic Data\n",
    "We'll create synthetic data with 3 features, where only 2 are relevant for prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a983070b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed for reproducibility\n",
    "np.random.seed(0)\n",
    "\n",
    "# Number of samples and features\n",
    "n_samples = 100\n",
    "n_features = 3\n",
    "\n",
    "# Generate features\n",
    "X1 = np.random.randn(n_samples)+5  # Shifted distribution\n",
    "X2 = np.random.randn(n_samples)\n",
    "X3 = np.random.randn(n_samples)  # Irrelevant feature\n",
    "\n",
    "# True coefficients (only X1 and X2 are relevant)\n",
    "beta_true = np.array([5, -3, 0])  # β3 = 0\n",
    "\n",
    "# Generate target variable with noise\n",
    "noise = np.random.randn(n_samples) * 0.5\n",
    "y = beta_true[0]*X1 + beta_true[1]*X2 + beta_true[2]*X3 + noise\n",
    "\n",
    "# Combine features into a matrix\n",
    "X = np.column_stack((X1, X2, X3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd468bdf",
   "metadata": {},
   "source": [
    "## 2. Visualize the Data\n",
    "Let's plot each feature against the target variable to visualize relationships."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed2a2e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 4))\n",
    "\n",
    "for i in range(n_features):\n",
    "    plt.subplot(1, n_features, i+1)\n",
    "    plt.scatter(X[:, i], y, alpha=0.7)\n",
    "    plt.xlabel(f'X{i+1}')\n",
    "    plt.ylabel('y')\n",
    "    plt.title(f'Feature X{i+1} vs y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15dd031f",
   "metadata": {},
   "source": [
    "## 3. Define Lasso Functions\n",
    "We implement the core Lasso regression functions:\n",
    "1. Soft thresholding operator\n",
    "2. Coordinate descent optimization\n",
    "3. Lasso with intercept handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c8d509e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def soft_thresholding(rho, lambda_):\n",
    "    \"\"\"Soft thresholding operator for Lasso.\"\"\"\n",
    "    if rho < -lambda_:\n",
    "        return rho + lambda_\n",
    "    elif rho > lambda_:\n",
    "        return rho - lambda_\n",
    "    else:\n",
    "        return 0.0\n",
    "\n",
    "def lasso_coordinate_descent(X, y, lambda_, num_iters=10000, tol=1e-6):\n",
    "    \"\"\"\n",
    "    Perform Lasso regression using Coordinate Descent.\n",
    "    \n",
    "    Parameters:\n",
    "    - X: Feature matrix (n_samples x n_features)\n",
    "    - y: Target vector (n_samples,)\n",
    "    - lambda_: Regularization parameter\n",
    "    - num_iters: Maximum number of iterations\n",
    "    - tol: Tolerance for convergence\n",
    "    \n",
    "    Returns:\n",
    "    - beta: Estimated coefficients (n_features,)\n",
    "    \"\"\"\n",
    "    n_samples, n_features = X.shape\n",
    "    beta = np.zeros(n_features)\n",
    "    \n",
    "    for iteration in range(num_iters):\n",
    "        beta_old = beta.copy()\n",
    "        \n",
    "        for j in range(n_features):\n",
    "            # Compute partial residual (excluding feature j)\n",
    "            residual = y - X @ beta + X[:, j] * beta[j]\n",
    "            \n",
    "            # Compute rho\n",
    "            rho = np.dot(X[:, j], residual) / n_samples\n",
    "            \n",
    "            # Update beta[j] using soft thresholding\n",
    "            beta[j] = soft_thresholding(rho, lambda_)\n",
    "        \n",
    "        # Check convergence\n",
    "        if np.sum(np.abs(beta - beta_old)) < tol:\n",
    "            break\n",
    "    else:\n",
    "        print('Reached maximum iterations without full convergence.')\n",
    "    \n",
    "    return beta\n",
    "\n",
    "def lasso_with_intercept(X, y, lambda_, num_iters=10000, tol=1e-6):\n",
    "    \"\"\"\n",
    "    Fit Lasso model with intercept.\n",
    "    \n",
    "    Parameters:\n",
    "    - X: Feature matrix (n_samples x n_features)\n",
    "    - y: Target vector (n_samples,)\n",
    "    - lambda_: Regularization parameter\n",
    "    - num_iters: Maximum number of iterations\n",
    "    - tol: Tolerance for convergence\n",
    "    \n",
    "    Returns:\n",
    "    - beta_full: Estimated coefficients including intercept (n_features + 1,)\n",
    "    \"\"\"\n",
    "    # Center the data\n",
    "    X_mean = np.mean(X, axis=0)\n",
    "    y_mean = np.mean(y)\n",
    "    X_centered = X\n",
    "    y_centered = y\n",
    "    \n",
    "    # Perform Lasso Coordinate Descent\n",
    "    beta = lasso_coordinate_descent(X_centered, y_centered, lambda_, num_iters, tol)\n",
    "    \n",
    "    # Compute intercept\n",
    "    intercept = y_mean - np.dot(X_mean, beta)\n",
    "    \n",
    "    # Combine intercept and coefficients\n",
    "    beta_full = np.insert(beta, 0, intercept)\n",
    "    \n",
    "    return beta_full"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7537c65a",
   "metadata": {},
   "source": [
    "## 4. Cross-Validation Implementation\n",
    "We implement k-fold cross-validation to select the optimal regularization parameter λ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "153e4740",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validate_lasso(X, y, lambda_values, k=5):\n",
    "    \"\"\"\n",
    "    Perform k-fold cross-validation to select the optimal lambda.\n",
    "    \n",
    "    Parameters:\n",
    "    - X: Feature matrix (n_samples x n_features)\n",
    "    - y: Target vector (n_samples,)\n",
    "    - lambda_values: Array of lambda values to evaluate\n",
    "    - k: Number of folds\n",
    "    \n",
    "    Returns:\n",
    "    - lambda_opt: Lambda with the lowest average validation error\n",
    "    - validation_errors: Average validation errors for each lambda\n",
    "    \"\"\"\n",
    "    kf = KFold(n_splits=k, shuffle=True, random_state=42)\n",
    "    validation_errors = np.zeros(len(lambda_values))\n",
    "    \n",
    "    for idx, lambda_ in enumerate(lambda_values):\n",
    "        mse_folds = []\n",
    "        for train_index, val_index in kf.split(X):\n",
    "            X_train, X_val = X[train_index], X[val_index]\n",
    "            y_train, y_val = y[train_index], y[val_index]\n",
    "            \n",
    "            # Fit model on training data\n",
    "            beta = lasso_with_intercept(X_train, y_train, lambda_)\n",
    "            \n",
    "            # Predict on validation data\n",
    "            intercept = beta[0]\n",
    "            coefficients = beta[1:]\n",
    "            y_pred = intercept + X_val @ coefficients\n",
    "            \n",
    "            # Compute Mean Squared Error\n",
    "            mse = np.mean((y_val - y_pred) ** 2)\n",
    "            mse_folds.append(mse)\n",
    "        \n",
    "        # Average MSE across folds\n",
    "        validation_errors[idx] = np.mean(mse_folds)\n",
    "    \n",
    "    # Select lambda with minimum validation error\n",
    "    lambda_opt = lambda_values[np.argmin(validation_errors)]\n",
    "    \n",
    "    return lambda_opt, validation_errors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e841897",
   "metadata": {},
   "source": [
    "## 5. Model Selection\n",
    "We'll perform cross-validation to find the optimal regularization parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe443942",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a refined range of lambda values\n",
    "lambda_values = np.linspace(0.1, 10, 100)\n",
    "\n",
    "# Perform cross-validation to find the optimal lambda\n",
    "lambda_opt, validation_errors = cross_validate_lasso(X, y, lambda_values, k=5)\n",
    "\n",
    "print(f'Optimal lambda selected by cross-validation: {lambda_opt:.2f}')\n",
    "\n",
    "# Plot validation error vs lambda\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(lambda_values, validation_errors, marker='o')\n",
    "plt.xlabel('Lambda (λ)')\n",
    "plt.ylabel('Average Validation MSE')\n",
    "plt.title('Cross-Validation for Lambda Selection')\n",
    "plt.axvline(x=lambda_opt, color='r', linestyle='--', label=f'Optimal λ = {lambda_opt:.2f}')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "804f8abe",
   "metadata": {},
   "source": [
    "## 6. Final Model Fitting\n",
    "Now we'll fit the final model using the optimal λ value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6f1c88d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit Lasso with optimal lambda on the entire dataset\n",
    "beta_optimal = lasso_with_intercept(X, y, lambda_opt)\n",
    "\n",
    "print(\"\\nOptimal coefficients (including intercept):\")\n",
    "print(f\"Intercept: {beta_optimal[0]:.4f}\")\n",
    "for i in range(1, n_features + 1):\n",
    "    print(f\"β{i}: {beta_optimal[i]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9a705a2",
   "metadata": {},
   "source": [
    "## 7. Regularization Path\n",
    "Visualize how coefficients change with different λ values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "728f91ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate coefficients for different lambda values\n",
    "coefficients = []\n",
    "\n",
    "for lambda_ in lambda_values:\n",
    "    beta = lasso_with_intercept(X, y, lambda_)\n",
    "    coefficients.append(beta)\n",
    "\n",
    "coefficients = np.array(coefficients)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "# Plot coefficients (excluding intercept)\n",
    "for j in range(1, n_features + 1):\n",
    "    plt.plot(lambda_values, coefficients[:, j], label=f'β{j}')\n",
    "\n",
    "plt.xlabel('Lambda (λ)')\n",
    "plt.ylabel('Coefficient Value')\n",
    "plt.title('Lasso Regularization Path')\n",
    "plt.legend()\n",
    "plt.gca().invert_xaxis()  # Lambda decreases from left to right\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e7265f5",
   "metadata": {},
   "source": [
    "## 8. Model Evaluation\n",
    "Finally, let's evaluate the model by comparing predicted vs actual values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82f690b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute predictions\n",
    "intercept_opt = beta_optimal[0]\n",
    "beta_coeffs_opt = beta_optimal[1:]\n",
    "y_pred = intercept_opt + X @ beta_coeffs_opt\n",
    "\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.scatter(y, y_pred, alpha=0.7)\n",
    "plt.xlabel('Actual y')\n",
    "plt.ylabel('Predicted y')\n",
    "plt.title(f'Lasso Regression Fit (λ={lambda_opt:.2f})')\n",
    "plt.plot([y.min(), y.max()], [y.min(), y.max()], 'r--')  # Line y = x\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
