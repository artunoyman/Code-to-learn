{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b52a1e60",
   "metadata": {},
   "source": [
    "# Classification Metrics and Visualization\n",
    "\n",
    "This notebook demonstrates various classification metrics and their visualization techniques, focusing on:\n",
    "- Confusion Matrix\n",
    "- ROC Curve\n",
    "- Classification Report\n",
    "- Accuracy and F1 Score\n",
    "\n",
    "## Theory Overview\n",
    "\n",
    "### Confusion Matrix\n",
    "A confusion matrix is a table that visualizes the performance of a classification model:\n",
    "- True Positives (TP): Correctly predicted positive cases\n",
    "- True Negatives (TN): Correctly predicted negative cases\n",
    "- False Positives (FP): Incorrectly predicted positive cases (Type I error)\n",
    "- False Negatives (FN): Incorrectly predicted negative cases (Type II error)\n",
    "\n",
    "### ROC Curve\n",
    "The Receiver Operating Characteristic (ROC) curve:\n",
    "- Plots True Positive Rate vs False Positive Rate\n",
    "- Area Under Curve (AUC) measures overall classification performance\n",
    "- Perfect classifier: AUC = 1.0\n",
    "- Random classifier: AUC = 0.5\n",
    "\n",
    "### Other Metrics\n",
    "- Accuracy: Overall correctness (TP + TN) / Total\n",
    "- Precision: TP / (TP + FP)\n",
    "- Recall: TP / (TP + FN)\n",
    "- F1 Score: Harmonic mean of precision and recall"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "592a375d",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f4bce82",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix, \n",
    "    roc_curve, \n",
    "    accuracy_score, \n",
    "    f1_score,\n",
    "    classification_report,\n",
    "    roc_auc_score\n",
    ")\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "plt.style.use('seaborn')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fde8213",
   "metadata": {},
   "source": [
    "## 2. Generate Synthetic Dataset\n",
    "\n",
    "We'll create a synthetic classification dataset with:\n",
    "- 1000 samples\n",
    "- 20 features\n",
    "- 2 classes\n",
    "- 5 informative features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59d14b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic dataset\n",
    "X, y = make_classification(\n",
    "    n_samples=1000,\n",
    "    n_features=20,\n",
    "    n_informative=5,\n",
    "    n_redundant=5,\n",
    "    n_classes=2,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(\"Dataset shapes:\")\n",
    "print(f\"Training set: {X_train.shape}\")\n",
    "print(f\"Testing set: {X_test.shape}\")\n",
    "print(f\"Class distribution in training set:\")\n",
    "print(np.bincount(y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03808cc3",
   "metadata": {},
   "source": [
    "## 3. Train Random Forest Classifier\n",
    "\n",
    "We'll use Random Forest as our classifier and get both class predictions and probability estimates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84fe7777",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and train the model\n",
    "rf_classifier = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=5,\n",
    "    random_state=42\n",
    ")\n",
    "rf_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Get predictions and probabilities\n",
    "y_pred = rf_classifier.predict(X_test)\n",
    "y_pred_proba = rf_classifier.predict_proba(X_test)[:, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75e0a29b",
   "metadata": {},
   "source": [
    "## 4. Confusion Matrix Visualization\n",
    "\n",
    "Let's create a heatmap visualization of the confusion matrix with annotations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e654492",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(y_true, y_pred, classes=['Class 0', 'Class 1']):\n",
    "    \"\"\"\n",
    "    Plot confusion matrix as a heatmap with annotations\n",
    "    \"\"\"\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap='Blues')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.colorbar()\n",
    "    \n",
    "    # Add labels\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "    \n",
    "    # Add text annotations\n",
    "    thresh = cm.max() / 2\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            plt.text(j, i, format(cm[i, j], 'd'),\n",
    "                    ha=\"center\", va=\"center\",\n",
    "                    color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    \n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Plot confusion matrix\n",
    "plot_confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Print classification metrics\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "258300f7",
   "metadata": {},
   "source": [
    "## 5. ROC Curve Visualization\n",
    "\n",
    "The ROC curve shows the trade-off between:\n",
    "- True Positive Rate (Sensitivity)\n",
    "- False Positive Rate (1 - Specificity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adf87ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_roc_curve(y_true, y_pred_proba):\n",
    "    \"\"\"\n",
    "    Plot ROC curve with AUC score\n",
    "    \"\"\"\n",
    "    fpr, tpr, _ = roc_curve(y_true, y_pred_proba)\n",
    "    auc = roc_auc_score(y_true, y_pred_proba)\n",
    "    \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(fpr, tpr, color='darkorange', lw=2, \n",
    "             label=f'ROC curve (AUC = {auc:.2f})')\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--',\n",
    "             label='Random classifier')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "# Plot ROC curve\n",
    "plot_roc_curve(y_test, y_pred_proba)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "769516dd",
   "metadata": {},
   "source": [
    "## 6. Performance Analysis\n",
    "\n",
    "Let's analyze the model's performance using various metrics and visualize the feature importances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55636373",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate and display metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Model Performance Metrics:\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "\n",
    "# Plot feature importances\n",
    "feature_importance = rf_classifier.feature_importances_\n",
    "feature_indices = np.argsort(feature_importance)[::-1]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(range(X.shape[1]), feature_importance[feature_indices])\n",
    "plt.title('Feature Importances')\n",
    "plt.xlabel('Feature Index')\n",
    "plt.ylabel('Importance')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
