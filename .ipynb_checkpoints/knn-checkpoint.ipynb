{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fb3e9673",
   "metadata": {},
   "source": [
    "# k-Nearest Neighbors (kNN) Implementation from Scratch\n",
    "\n",
    "This notebook demonstrates the implementation of the k-Nearest Neighbors algorithm from scratch. kNN is a simple yet powerful machine learning algorithm used for both classification and regression tasks.\n",
    "\n",
    "## Theory Overview\n",
    "\n",
    "### What is kNN?\n",
    "k-Nearest Neighbors is a non-parametric, instance-based learning algorithm that:\n",
    "1. Stores all training examples in memory\n",
    "2. Makes predictions by finding the k closest training examples\n",
    "3. Uses majority voting (for classification) or averaging (for regression)\n",
    "\n",
    "### Key Concepts\n",
    "- **Distance Metrics**: Ways to measure similarity between points (Euclidean, Manhattan, etc.)\n",
    "- **k Parameter**: Number of neighbors to consider\n",
    "- **Voting Schemes**: How to combine neighbor information\n",
    "- **Curse of Dimensionality**: Performance degradation in high dimensions\n",
    "\n",
    "### Advantages and Disadvantages\n",
    "Advantages:\n",
    "- Simple to understand and implement\n",
    "- No training phase (lazy learning)\n",
    "- Can model complex decision boundaries\n",
    "- Works well with multi-class problems\n",
    "\n",
    "Disadvantages:\n",
    "- Computationally expensive during prediction\n",
    "- Requires large memory to store training data\n",
    "- Sensitive to irrelevant features\n",
    "- Needs feature scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbbf1ce8",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c6684fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "plt.style.use('seaborn')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9359448",
   "metadata": {},
   "source": [
    "## 2. KNN Classifier Implementation\n",
    "\n",
    "We'll implement a KNN classifier with the following features:\n",
    "- Euclidean distance calculation\n",
    "- k-nearest neighbors search\n",
    "- Majority voting prediction\n",
    "- Support for multiple distance metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3895a3aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KNNClassifier:\n",
    "    def __init__(self, k=3, metric='euclidean'):\n",
    "        \"\"\"\n",
    "        Initialize KNN Classifier\n",
    "        \n",
    "        Parameters:\n",
    "            k: Number of neighbors to consider\n",
    "            metric: Distance metric ('euclidean' or 'manhattan')\n",
    "        \"\"\"\n",
    "        self.k = k\n",
    "        self.metric = metric\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Store training data\n",
    "        \n",
    "        Parameters:\n",
    "            X: Training features\n",
    "            y: Training labels\n",
    "        \"\"\"\n",
    "        self.X_train = X\n",
    "        self.y_train = y\n",
    "    \n",
    "    def euclidean_distance(self, x1, x2):\n",
    "        \"\"\"Calculate Euclidean distance between two points\"\"\"\n",
    "        return np.sqrt(np.sum((x1 - x2) ** 2))\n",
    "    \n",
    "    def manhattan_distance(self, x1, x2):\n",
    "        \"\"\"Calculate Manhattan distance between two points\"\"\"\n",
    "        return np.sum(np.abs(x1 - x2))\n",
    "    \n",
    "    def get_distance(self, x1, x2):\n",
    "        \"\"\"Get distance based on selected metric\"\"\"\n",
    "        if self.metric == 'euclidean':\n",
    "            return self.euclidean_distance(x1, x2)\n",
    "        elif self.metric == 'manhattan':\n",
    "            return self.manhattan_distance(x1, x2)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown metric: {self.metric}\")\n",
    "    \n",
    "    def get_neighbors(self, x):\n",
    "        \"\"\"Find k nearest neighbors of a point\"\"\"\n",
    "        # Calculate distances to all training points\n",
    "        distances = [self.get_distance(x, x_train) for x_train in self.X_train]\n",
    "        \n",
    "        # Get indices of k nearest neighbors\n",
    "        k_indices = np.argsort(distances)[:self.k]\n",
    "        \n",
    "        # Return the corresponding labels\n",
    "        return self.y_train[k_indices]\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Make predictions for multiple samples\n",
    "        \n",
    "        Parameters:\n",
    "            X: Samples to predict\n",
    "            \n",
    "        Returns:\n",
    "            predictions: Predicted labels\n",
    "        \"\"\"\n",
    "        predictions = []\n",
    "        for x in X:\n",
    "            # Get k nearest neighbors\n",
    "            neighbors = self.get_neighbors(x)\n",
    "            \n",
    "            # Make prediction by majority voting\n",
    "            most_common = Counter(neighbors).most_common(1)\n",
    "            predictions.append(most_common[0][0])\n",
    "        \n",
    "        return np.array(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "998dee86",
   "metadata": {},
   "source": [
    "## 3. Generate Synthetic Dataset\n",
    "\n",
    "We'll create a synthetic classification dataset with clear class separation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37879fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic data\n",
    "X, y = make_classification(\n",
    "    n_samples=300,\n",
    "    n_features=2,\n",
    "    n_redundant=0,\n",
    "    n_informative=2,\n",
    "    random_state=42,\n",
    "    n_clusters_per_class=1,\n",
    "    class_sep=0.5\n",
    ")\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Plot training data\n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.scatter(X_train[y_train == 0][:, 0], X_train[y_train == 0][:, 1],\n",
    "           label='Class 0', alpha=0.6)\n",
    "plt.scatter(X_train[y_train == 1][:, 0], X_train[y_train == 1][:, 1],\n",
    "           label='Class 1', alpha=0.6)\n",
    "plt.title('Original Training Data')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.scatter(X_train_scaled[y_train == 0][:, 0], X_train_scaled[y_train == 0][:, 1],\n",
    "           label='Class 0', alpha=0.6)\n",
    "plt.scatter(X_train_scaled[y_train == 1][:, 0], X_train_scaled[y_train == 1][:, 1],\n",
    "           label='Class 1', alpha=0.6)\n",
    "plt.title('Scaled Training Data')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e09e47e",
   "metadata": {},
   "source": [
    "## 4. Model Training and Evaluation\n",
    "\n",
    "Let's train and evaluate our KNN classifier with different values of k."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b19e832",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_knn(k_values, X_train, X_test, y_train, y_test):\n",
    "    \"\"\"Evaluate KNN with different k values\"\"\"\n",
    "    train_scores = []\n",
    "    test_scores = []\n",
    "    \n",
    "    for k in k_values:\n",
    "        # Create and train model\n",
    "        knn = KNNClassifier(k=k)\n",
    "        knn.fit(X_train, y_train)\n",
    "        \n",
    "        # Make predictions\n",
    "        train_pred = knn.predict(X_train)\n",
    "        test_pred = knn.predict(X_test)\n",
    "        \n",
    "        # Calculate accuracy\n",
    "        train_acc = np.mean(train_pred == y_train)\n",
    "        test_acc = np.mean(test_pred == y_test)\n",
    "        \n",
    "        train_scores.append(train_acc)\n",
    "        test_scores.append(test_acc)\n",
    "    \n",
    "    return train_scores, test_scores\n",
    "\n",
    "# Evaluate different k values\n",
    "k_values = [1, 3, 5, 7, 9, 11, 13, 15]\n",
    "train_scores, test_scores = evaluate_knn(k_values, X_train_scaled, X_test_scaled, \n",
    "                                       y_train, y_test)\n",
    "\n",
    "# Plot results\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(k_values, train_scores, marker='o', label='Training Accuracy')\n",
    "plt.plot(k_values, test_scores, marker='s', label='Testing Accuracy')\n",
    "plt.xlabel('k (Number of Neighbors)')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('KNN Performance vs k Value')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Print best k value\n",
    "best_k = k_values[np.argmax(test_scores)]\n",
    "print(f\"Best k value: {best_k}\")\n",
    "print(f\"Best test accuracy: {max(test_scores):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "924bce87",
   "metadata": {},
   "source": [
    "## 5. Decision Boundary Visualization\n",
    "\n",
    "Let's visualize the decision boundaries created by our KNN classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38645751",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_decision_boundary(model, X, y, title):\n",
    "    \"\"\"Plot decision boundary and data points\"\"\"\n",
    "    h = 0.02  # step size in the mesh\n",
    "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                         np.arange(y_min, y_max, h))\n",
    "\n",
    "    # Make predictions on the mesh\n",
    "    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "\n",
    "    # Plot the decision boundary\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.contourf(xx, yy, Z, alpha=0.4)\n",
    "    plt.scatter(X[y == 0][:, 0], X[y == 0][:, 1], \n",
    "               label='Class 0', alpha=0.6)\n",
    "    plt.scatter(X[y == 1][:, 0], X[y == 1][:, 1], \n",
    "               label='Class 1', alpha=0.6)\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Feature 1')\n",
    "    plt.ylabel('Feature 2')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# Create and train model with best k\n",
    "best_knn = KNNClassifier(k=best_k)\n",
    "best_knn.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Plot decision boundary\n",
    "plot_decision_boundary(best_knn, X_train_scaled, y_train, \n",
    "                      f'KNN Decision Boundary (k={best_k})')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c1e92b7",
   "metadata": {},
   "source": [
    "## 6. Distance Metrics Comparison\n",
    "\n",
    "Finally, let's compare the performance of different distance metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1943fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_metrics(X_train, X_test, y_train, y_test, k):\n",
    "    \"\"\"Compare different distance metrics\"\"\"\n",
    "    metrics = ['euclidean', 'manhattan']\n",
    "    results = {}\n",
    "    \n",
    "    for metric in metrics:\n",
    "        # Create and train model\n",
    "        knn = KNNClassifier(k=k, metric=metric)\n",
    "        knn.fit(X_train, y_train)\n",
    "        \n",
    "        # Make predictions\n",
    "        train_pred = knn.predict(X_train)\n",
    "        test_pred = knn.predict(X_test)\n",
    "        \n",
    "        # Calculate accuracy\n",
    "        train_acc = np.mean(train_pred == y_train)\n",
    "        test_acc = np.mean(test_pred == y_test)\n",
    "        \n",
    "        results[metric] = {'train': train_acc, 'test': test_acc}\n",
    "        \n",
    "        # Plot decision boundary\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plot_decision_boundary(knn, X_train, y_train,\n",
    "                             f'KNN Decision Boundary ({metric} distance, k={k})')\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Compare metrics\n",
    "metric_results = compare_metrics(X_train_scaled, X_test_scaled, y_train, y_test, best_k)\n",
    "\n",
    "# Print results\n",
    "print(\"\\nDistance Metrics Comparison:\")\n",
    "for metric, scores in metric_results.items():\n",
    "    print(f\"\\n{metric.capitalize()} distance:\")\n",
    "    print(f\"Training accuracy: {scores['train']:.4f}\")\n",
    "    print(f\"Testing accuracy: {scores['test']:.4f}\")"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
