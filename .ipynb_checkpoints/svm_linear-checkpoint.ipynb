{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "42a65e42",
   "metadata": {},
   "source": [
    "# Linear Support Vector Machine Implementation using Quadratic Programming\n",
    "\n",
    "This notebook implements a Linear Support Vector Machine (SVM) classifier from scratch using Quadratic Programming (QP) optimization. The implementation includes:\n",
    "\n",
    "1. Custom SVM class using CVXOPT for QP optimization\n",
    "2. Linear kernel implementation\n",
    "3. Support vector visualization\n",
    "4. Distance calculations to decision boundary\n",
    "5. Detailed visualization of the decision boundary and support vectors\n",
    "\n",
    "The goal is to demonstrate how SVM works internally and visualize its key components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0634f8fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from cvxopt import matrix, solvers  # Import QP solver"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a11b89d",
   "metadata": {},
   "source": [
    "## SVM Implementation using Quadratic Programming\n",
    "\n",
    "The SVM implementation includes:\n",
    "- Linear kernel computation\n",
    "- QP formulation for optimization\n",
    "- Support vector identification\n",
    "- Decision boundary computation\n",
    "- Distance calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f70e158a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SVM_QP:\n",
    "    def __init__(self, C=1.0):\n",
    "        self.C = C\n",
    "        self.alpha = None\n",
    "        self.b = 0\n",
    "        self.w = None\n",
    "        self.X_train = None\n",
    "        self.y_ = None\n",
    "\n",
    "    def linear_kernel(self, x1, x2):\n",
    "        return np.dot(x1, x2)\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.X_train = X\n",
    "        self.y_ = np.where(y <= 0, -1, 1)\n",
    "\n",
    "        n_samples, n_features = X.shape\n",
    "\n",
    "        # Compute Kernel Matrix\n",
    "        K = np.dot(X, X.T)  # Linear kernel\n",
    "\n",
    "        # Quadratic Programming Formulation\n",
    "        P = matrix(np.outer(self.y_, self.y_) * K)  # Quadratic term\n",
    "        q = matrix(-np.ones(n_samples))  # Linear term\n",
    "\n",
    "        G = matrix(np.vstack((-np.eye(n_samples), np.eye(n_samples))))  # Inequality constraints\n",
    "        h = matrix(np.hstack((np.zeros(n_samples), self.C * np.ones(n_samples))))  # Bounds for alpha\n",
    "\n",
    "        A = matrix(self.y_.astype(float), (1, n_samples))  # Equality constraint\n",
    "        b = matrix(0.0)\n",
    "\n",
    "        # Solve QP Problem\n",
    "        solvers.options['show_progress'] = False\n",
    "        solution = solvers.qp(P, q, G, h, A, b)\n",
    "        self.alpha = np.ravel(solution['x'])\n",
    "\n",
    "        # Compute weight vector\n",
    "        self.w = np.sum(self.alpha[:, None] * self.y_[:, None] * X, axis=0)\n",
    "\n",
    "        # Compute bias using support vectors\n",
    "        sv_indices = np.where((self.alpha > 1e-3) & (self.alpha < self.C - 1e-3))[0]\n",
    "        if len(sv_indices) > 0:\n",
    "            self.b = np.mean(self.y_[sv_indices] - np.dot(X[sv_indices], self.w))\n",
    "        else:\n",
    "            self.b = 0\n",
    "\n",
    "    def predict(self, X):\n",
    "        return np.sign(np.dot(X, self.w) + self.b)\n",
    "\n",
    "    def decision_boundary_distance(self, X):\n",
    "        \"\"\" Compute distance of points to the decision boundary \"\"\"\n",
    "        return np.abs(np.dot(X, self.w) + self.b) / np.linalg.norm(self.w)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cebd4945",
   "metadata": {},
   "source": [
    "## Data Generation\n",
    "\n",
    "We'll generate linearly separable data with two classes:\n",
    "- Class 0: Points around (1,1)\n",
    "- Class 1: Points around (5,5)\n",
    "- Gaussian noise added with Ïƒ=0.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fcd99bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_linear_data(n=20, test_ratio=0.2):\n",
    "    data = []\n",
    "    for _ in range(2 * n):\n",
    "        x = 1 + random.gauss(0, 0.4)\n",
    "        y = 1 + random.gauss(0, 0.4)\n",
    "        data.append(([x, y], 0))\n",
    "    for _ in range(2 * n):\n",
    "        x = 5 + random.gauss(0, 0.4)\n",
    "        y = 5 + random.gauss(0, 0.4)\n",
    "        data.append(([x, y], 1))\n",
    "    random.shuffle(data)\n",
    "    split_idx = int(len(data) * (1 - test_ratio))\n",
    "    return data[:split_idx], data[split_idx:]\n",
    "\n",
    "# Generate and split data\n",
    "train_data, test_data = generate_linear_data(n=10, test_ratio=0.2)\n",
    "X_train = np.array([d[0] for d in train_data])\n",
    "y_train = np.array([d[1] for d in train_data])\n",
    "X_test = np.array([d[0] for d in test_data])\n",
    "y_test = np.array([d[1] for d in test_data])\n",
    "\n",
    "# Visualize the raw data\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=plt.cm.Paired, edgecolors='k')\n",
    "plt.title(\"Generated Linear Data\")\n",
    "plt.xlabel(\"Feature 1\")\n",
    "plt.ylabel(\"Feature 2\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7609b194",
   "metadata": {},
   "source": [
    "## Model Training and Evaluation\n",
    "\n",
    "Let's train our SVM model and evaluate its performance on both training and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68e40e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train SVM using QP\n",
    "svm = SVM_QP(C=1.0)\n",
    "svm.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate\n",
    "train_pred = (svm.predict(X_train) + 1) // 2\n",
    "test_pred = (svm.predict(X_test) + 1) // 2\n",
    "train_accuracy = np.mean(train_pred == y_train)\n",
    "test_accuracy = np.mean(test_pred == y_test)\n",
    "print(f\"Training Accuracy: {train_accuracy:.2f}\")\n",
    "print(f\"Test Accuracy: {test_accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c740b46",
   "metadata": {},
   "source": [
    "## Decision Boundary and Support Vector Visualization\n",
    "\n",
    "We'll create a detailed visualization showing:\n",
    "1. Decision boundary\n",
    "2. All training points\n",
    "3. Support vectors (points closest to the boundary)\n",
    "4. Distance measurements from support vectors to the boundary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da4bd09d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute distances to decision boundary\n",
    "distances = svm.decision_boundary_distance(X_train)\n",
    "\n",
    "# Select 3 closest points from each class\n",
    "class_0_idx = np.where(y_train == 0)[0]\n",
    "class_1_idx = np.where(y_train == 1)[0]\n",
    "\n",
    "closest_0 = class_0_idx[np.argsort(distances[class_0_idx])[:3]]\n",
    "closest_1 = class_1_idx[np.argsort(distances[class_1_idx])[:3]]\n",
    "\n",
    "# Create meshgrid for visualization\n",
    "xx, yy = np.meshgrid(np.linspace(0, 6, 200), np.linspace(0, 6, 200))\n",
    "Z = svm.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "Z = (Z + 1) // 2\n",
    "Z = Z.reshape(xx.shape)\n",
    "\n",
    "# Plot decision boundary with training data\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.contourf(xx, yy, Z, alpha=0.3, cmap=plt.cm.Paired)\n",
    "plt.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=plt.cm.Paired, edgecolors='k', s=50)\n",
    "\n",
    "# Draw lines from support vectors to decision boundary\n",
    "for idx in np.concatenate((closest_0, closest_1)):\n",
    "    point = X_train[idx]\n",
    "    \n",
    "    # Compute the projection of the point onto the decision boundary\n",
    "    proj = point - (np.dot(point, svm.w) + svm.b) / np.dot(svm.w, svm.w) * svm.w\n",
    "\n",
    "    # Draw line\n",
    "    plt.plot([point[0], proj[0]], [point[1], proj[1]], 'r--', linewidth=2)\n",
    "\n",
    "    # Annotate with distance\n",
    "    dist = distances[idx]\n",
    "    mid_x = (point[0] + proj[0]) / 2\n",
    "    mid_y = (point[1] + proj[1]) / 2\n",
    "    plt.text(mid_x, mid_y, f\"{dist:.2f}\", fontsize=12, color='red', ha='center', va='center')\n",
    "\n",
    "plt.title(\"SVM Decision Boundary with Support Vectors\")\n",
    "plt.xlabel(\"Feature 1\")\n",
    "plt.ylabel(\"Feature 2\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2a26f4a",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "\n",
    "This implementation demonstrates:\n",
    "1. How SVM finds the optimal separating hyperplane using Quadratic Programming\n",
    "2. The role of support vectors in defining the decision boundary\n",
    "3. How distances to the decision boundary are calculated\n",
    "4. The effectiveness of SVM for linearly separable data\n",
    "\n",
    "The visualization shows both the decision boundary and the actual margin width through the distance measurements to the closest points (support vectors)."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
