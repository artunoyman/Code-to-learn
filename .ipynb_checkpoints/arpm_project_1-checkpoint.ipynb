{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "773dd2a7",
   "metadata": {},
   "source": [
    "# Simple Linear Regression with Gradient Descent\n",
    "\n",
    "This notebook demonstrates the implementation of Simple Linear Regression using Gradient Descent optimization. We'll build the algorithm from scratch to understand the underlying mathematics and optimization process.\n",
    "\n",
    "## Theory Overview\n",
    "\n",
    "### Linear Regression\n",
    "Linear regression models the relationship between a dependent variable y and an independent variable x using a linear equation:\n",
    "```\n",
    "y = θ₀ + θ₁x\n",
    "```\n",
    "where:\n",
    "- θ₀ (theta_0) is the y-intercept\n",
    "- θ₁ (theta_1) is the slope\n",
    "\n",
    "### Gradient Descent\n",
    "Gradient Descent is an iterative optimization algorithm that:\n",
    "1. Starts with initial parameter values\n",
    "2. Computes the gradient of the cost function\n",
    "3. Updates parameters in the direction that reduces the cost\n",
    "4. Repeats until convergence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0c99a84",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee47d2a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Set plot style\n",
    "plt.style.use('seaborn')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06057ece",
   "metadata": {},
   "source": [
    "## 2. Generate Synthetic Data\n",
    "\n",
    "We'll create synthetic data with a known relationship plus some noise to simulate real-world data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90998d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic data\n",
    "def generate_data(n_samples=100, noise_std=2):\n",
    "    \"\"\"\n",
    "    Generate synthetic data for linear regression\n",
    "    \n",
    "    Parameters:\n",
    "        n_samples: Number of data points\n",
    "        noise_std: Standard deviation of Gaussian noise\n",
    "    \n",
    "    Returns:\n",
    "        X: Feature values\n",
    "        y: Target values\n",
    "    \"\"\"\n",
    "    # True parameters\n",
    "    true_theta0 = 2\n",
    "    true_theta1 = 0.5\n",
    "    \n",
    "    # Generate X values\n",
    "    X = np.linspace(0, 10, n_samples)\n",
    "    \n",
    "    # Generate y values with noise\n",
    "    y = true_theta0 + true_theta1 * X + np.random.normal(0, noise_std, n_samples)\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "# Generate data\n",
    "X, y = generate_data()\n",
    "\n",
    "# Visualize the data\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(X, y, alpha=0.5, label='Data points')\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('y')\n",
    "plt.title('Synthetic Data for Linear Regression')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9e45e19",
   "metadata": {},
   "source": [
    "## 3. Cost Function Implementation\n",
    "\n",
    "The Mean Squared Error (MSE) cost function measures how well our model fits the data:\n",
    "\n",
    "$J(θ₀, θ₁) = \\frac{1}{2m} \\sum_{i=1}^m (h_{θ}(x^{(i)}) - y^{(i)})^2$\n",
    "\n",
    "where:\n",
    "- m is the number of training examples\n",
    "- h_{θ}(x) = θ₀ + θ₁x is our hypothesis function\n",
    "- y^{(i)} is the actual target value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d45961c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(X, y, theta0, theta1):\n",
    "    \"\"\"\n",
    "    Compute the Mean Squared Error cost\n",
    "    \n",
    "    Parameters:\n",
    "        X: Feature values\n",
    "        y: Target values\n",
    "        theta0: y-intercept parameter\n",
    "        theta1: slope parameter\n",
    "    \n",
    "    Returns:\n",
    "        cost: Mean Squared Error\n",
    "    \"\"\"\n",
    "    m = len(y)\n",
    "    predictions = theta0 + theta1 * X\n",
    "    cost = (1/(2*m)) * np.sum((predictions - y)**2)\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9281c0d1",
   "metadata": {},
   "source": [
    "## 4. Gradient Descent Implementation\n",
    "\n",
    "The gradient descent update rules for θ₀ and θ₁ are:\n",
    "\n",
    "$θ₀ := θ₀ - α \\frac{1}{m} \\sum_{i=1}^m (h_{θ}(x^{(i)}) - y^{(i)})$\n",
    "\n",
    "$θ₁ := θ₁ - α \\frac{1}{m} \\sum_{i=1}^m (h_{θ}(x^{(i)}) - y^{(i)})x^{(i)}$\n",
    "\n",
    "where α is the learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60c4ce39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(X, y, theta0, theta1, alpha, num_iters):\n",
    "    \"\"\"\n",
    "    Perform gradient descent to optimize theta0 and theta1\n",
    "    \n",
    "    Parameters:\n",
    "        X: Feature values\n",
    "        y: Target values\n",
    "        theta0: Initial y-intercept parameter\n",
    "        theta1: Initial slope parameter\n",
    "        alpha: Learning rate\n",
    "        num_iters: Number of iterations\n",
    "    \n",
    "    Returns:\n",
    "        theta0: Optimized y-intercept parameter\n",
    "        theta1: Optimized slope parameter\n",
    "        cost_history: List of costs at each iteration\n",
    "    \"\"\"\n",
    "    m = len(y)\n",
    "    cost_history = []\n",
    "    \n",
    "    for _ in range(num_iters):\n",
    "        # Compute predictions\n",
    "        predictions = theta0 + theta1 * X\n",
    "        \n",
    "        # Compute gradients\n",
    "        grad_theta0 = (1/m) * np.sum(predictions - y)\n",
    "        grad_theta1 = (1/m) * np.sum((predictions - y) * X)\n",
    "        \n",
    "        # Update parameters\n",
    "        theta0 = theta0 - alpha * grad_theta0\n",
    "        theta1 = theta1 - alpha * grad_theta1\n",
    "        \n",
    "        # Compute and store cost\n",
    "        cost = compute_cost(X, y, theta0, theta1)\n",
    "        cost_history.append(cost)\n",
    "    \n",
    "    return theta0, theta1, cost_history\n",
    "\n",
    "# Train the model\n",
    "initial_theta0 = 0\n",
    "initial_theta1 = 0\n",
    "alpha = 0.01\n",
    "num_iters = 1000\n",
    "\n",
    "theta0, theta1, cost_history = gradient_descent(X, y, initial_theta0, initial_theta1, alpha, num_iters)\n",
    "\n",
    "print(f'Optimized parameters:')\n",
    "print(f'θ₀ (y-intercept): {theta0:.4f}')\n",
    "print(f'θ₁ (slope): {theta1:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c6be96b",
   "metadata": {},
   "source": [
    "## 5. Results Visualization\n",
    "\n",
    "Let's visualize:\n",
    "1. The convergence of the cost function\n",
    "2. The final regression line against the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51954311",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot cost history\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(cost_history)\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Cost')\n",
    "plt.title('Cost Function Convergence')\n",
    "plt.grid(True)\n",
    "\n",
    "# Plot final regression line\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.scatter(X, y, alpha=0.5, label='Data points')\n",
    "plt.plot(X, theta0 + theta1 * X, 'r', label='Regression line')\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('y')\n",
    "plt.title('Linear Regression Result')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
