{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6414a533",
   "metadata": {},
   "source": [
    "# Understanding Multicollinearity in Linear Regression\n",
    "\n",
    "This notebook demonstrates the effects of multicollinearity in linear regression through:\n",
    "- Synthetic data generation with and without multicollinearity\n",
    "- Visualization of correlations\n",
    "- VIF (Variance Inflation Factor) analysis\n",
    "- Model fitting and comparison\n",
    "- Coefficient stability analysis using bootstrapping\n",
    "\n",
    "Originally created by hilmi, adapted to Jupyter notebook format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a19f4492",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Necessary Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from sklearn.utils import resample\n",
    "\n",
    "# Set plot style for better aesthetics\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Seed for reproducibility\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86d189d0",
   "metadata": {},
   "source": [
    "## 1. Generate Synthetic Data\n",
    "We'll create two datasets:\n",
    "1. Without multicollinearity: Independent predictors\n",
    "2. With multicollinearity: Highly correlated predictors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47f1177d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a. Without Multicollinearity (Independent Predictors)\n",
    "n_samples = 100\n",
    "X1 = np.random.normal(0, 1, n_samples)\n",
    "X2 = np.random.normal(0, 1, n_samples)\n",
    "y_no_multicol = 3 * X1 + 5 * X2 + np.random.normal(0, 1, n_samples)\n",
    "data_no_multicol = pd.DataFrame({'X1': X1, 'X2': X2, 'y': y_no_multicol})\n",
    "\n",
    "# b. With Multicollinearity (Highly Correlated Predictors)\n",
    "X1_mc = np.random.normal(0, 1, n_samples)\n",
    "X2_mc = X1_mc + np.random.normal(0, 0.1, n_samples)  # X2 is X1 plus small noise\n",
    "y_multicol = 3 * X1_mc + 5 * X2_mc + np.random.normal(0, 1, n_samples)\n",
    "data_multicol = pd.DataFrame({'X1': X1_mc, 'X2': X2_mc, 'y': y_multicol})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8554903",
   "metadata": {},
   "source": [
    "## 2. Visualize Correlation Matrices\n",
    "Let's compare the correlation matrices of both datasets to visualize the relationship between predictors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08cb98b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# a. No Multicollinearity\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.heatmap(data_no_multicol.corr(), annot=True, cmap='coolwarm', vmin=-1, vmax=1)\n",
    "plt.title('Correlation Matrix (No Multicollinearity)')\n",
    "\n",
    "# b. With Multicollinearity\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.heatmap(data_multicol.corr(), annot=True, cmap='coolwarm', vmin=-1, vmax=1)\n",
    "plt.title('Correlation Matrix (With Multicollinearity)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2008129d",
   "metadata": {},
   "source": [
    "## 3. Variance Inflation Factor (VIF) Analysis\n",
    "VIF quantifies the severity of multicollinearity. A VIF value:\n",
    "- = 1: Variables are uncorrelated\n",
    "- 1-5: Moderate correlation\n",
    "- > 5: High correlation (problematic)\n",
    "- > 10: Severe multicollinearity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bf3fa75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_vif(df):\n",
    "    X = df[['X1', 'X2']]\n",
    "    X = sm.add_constant(X)\n",
    "    vif_data = pd.DataFrame()\n",
    "    vif_data['Feature'] = ['const'] + ['X1', 'X2']\n",
    "    vif_data['VIF'] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n",
    "    return vif_data\n",
    "\n",
    "# a. VIF Without Multicollinearity\n",
    "vif_no_multicol = calculate_vif(data_no_multicol)\n",
    "print(\"Variance Inflation Factor (No Multicollinearity):\")\n",
    "print(vif_no_multicol)\n",
    "\n",
    "# b. VIF With Multicollinearity\n",
    "vif_with_multicol = calculate_vif(data_multicol)\n",
    "print(\"\\nVariance Inflation Factor (With Multicollinearity):\")\n",
    "print(vif_with_multicol)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df833fa7",
   "metadata": {},
   "source": [
    "## 4. Fit Linear Regression Models\n",
    "We'll fit models to both datasets using scikit-learn and statsmodels to compare the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fe8519d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a. Without Multicollinearity\n",
    "X_no_mc = data_no_multicol[['X1', 'X2']]\n",
    "y_no_mc = data_no_multicol['y']\n",
    "\n",
    "# Using scikit-learn\n",
    "lr_no_mc = LinearRegression().fit(X_no_mc, y_no_mc)\n",
    "y_pred_no_mc = lr_no_mc.predict(X_no_mc)\n",
    "mse_no_mc = mean_squared_error(y_no_mc, y_pred_no_mc)\n",
    "\n",
    "print(\"Linear Regression (No Multicollinearity) - scikit-learn:\")\n",
    "print(f\"Coefficients: {lr_no_mc.coef_}\")\n",
    "print(f\"Intercept: {lr_no_mc.intercept_}\")\n",
    "print(f\"Mean Squared Error: {mse_no_mc:.4f}\")\n",
    "\n",
    "# Using statsmodels\n",
    "X_sm_no_mc = sm.add_constant(X_no_mc)\n",
    "model_no_mc = sm.OLS(y_no_mc, X_sm_no_mc).fit()\n",
    "print(\"\\nStatsmodels Summary (No Multicollinearity):\")\n",
    "print(model_no_mc.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c50274ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# b. With Multicollinearity\n",
    "X_mc = data_multicol[['X1', 'X2']]\n",
    "y_mc = data_multicol['y']\n",
    "\n",
    "# Using scikit-learn\n",
    "lr_mc = LinearRegression().fit(X_mc, y_mc)\n",
    "y_pred_mc = lr_mc.predict(X_mc)\n",
    "mse_mc = mean_squared_error(y_mc, y_pred_mc)\n",
    "\n",
    "print(\"Linear Regression (With Multicollinearity) - scikit-learn:\")\n",
    "print(f\"Coefficients: {lr_mc.coef_}\")\n",
    "print(f\"Intercept: {lr_mc.intercept_}\")\n",
    "print(f\"Mean Squared Error: {mse_mc:.4f}\")\n",
    "\n",
    "# Using statsmodels\n",
    "X_sm_mc = sm.add_constant(X_mc)\n",
    "model_mc = sm.OLS(y_mc, X_sm_mc).fit()\n",
    "print(\"\\nStatsmodels Summary (With Multicollinearity):\")\n",
    "print(model_mc.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09f79942",
   "metadata": {},
   "source": [
    "## 5. Condition Number Analysis\n",
    "The condition number measures the numerical stability of the feature matrix. A high condition number indicates potential numerical problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "422d6b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def condition_number(df):\n",
    "    X = df[['X1', 'X2']].values\n",
    "    X = sm.add_constant(X)\n",
    "    return np.linalg.cond(X)\n",
    "\n",
    "cond_no_mc = condition_number(data_no_multicol)\n",
    "cond_with_mc = condition_number(data_multicol)\n",
    "\n",
    "print(f\"Condition Number (No Multicollinearity): {cond_no_mc:.2f}\")\n",
    "print(f\"Condition Number (With Multicollinearity): {cond_with_mc:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97c1e165",
   "metadata": {},
   "source": [
    "## 6. Bootstrapping Analysis\n",
    "We'll use bootstrapping to visualize the stability of coefficient estimates under both conditions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78e1b8f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bootstrap_coefficients(df, n_bootstrap=1000):\n",
    "    X = df[['X1', 'X2']]\n",
    "    y = df['y']\n",
    "    coefs = []\n",
    "    for _ in range(n_bootstrap):\n",
    "        X_res, y_res = resample(X, y)\n",
    "        lr = LinearRegression().fit(X_res, y_res)\n",
    "        coefs.append(lr.coef_)\n",
    "    return np.array(coefs)\n",
    "\n",
    "# Bootstrap without multicollinearity\n",
    "coefs_no_mc = bootstrap_coefficients(data_no_multicol)\n",
    "\n",
    "# Bootstrap with multicollinearity\n",
    "coefs_with_mc = bootstrap_coefficients(data_multicol)\n",
    "\n",
    "# Plotting the Bootstrapped Coefficients\n",
    "plt.figure(figsize=(14, 6))\n",
    "\n",
    "# a. No Multicollinearity\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(coefs_no_mc[:, 0], bins=30, alpha=0.7, label='X1')\n",
    "plt.hist(coefs_no_mc[:, 1], bins=30, alpha=0.7, label='X2')\n",
    "plt.axvline(3, color='blue', linestyle='dashed', linewidth=1)\n",
    "plt.axvline(5, color='orange', linestyle='dashed', linewidth=1)\n",
    "plt.title('Coefficient Distribution (No Multicollinearity)')\n",
    "plt.xlabel('Coefficient Value')\n",
    "plt.ylabel('Frequency')\n",
    "plt.legend()\n",
    "\n",
    "# b. With Multicollinearity\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.hist(coefs_with_mc[:, 0], bins=30, alpha=0.7, label='X1')\n",
    "plt.hist(coefs_with_mc[:, 1], bins=30, alpha=0.7, label='X2')\n",
    "plt.axvline(3, color='blue', linestyle='dashed', linewidth=1)\n",
    "plt.axvline(5, color='orange', linestyle='dashed', linewidth=1)\n",
    "plt.title('Coefficient Distribution (With Multicollinearity)')\n",
    "plt.xlabel('Coefficient Value')\n",
    "plt.ylabel('Frequency')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5de8945a",
   "metadata": {},
   "source": [
    "## 7. Summary of Findings\n",
    "\n",
    "1. **Correlation Matrix:**\n",
    "   - **No Multicollinearity:** Low correlation between X1 and X2.\n",
    "   - **With Multicollinearity:** High correlation between X1 and X2 (≈1).\n",
    "\n",
    "2. **Variance Inflation Factor (VIF):**\n",
    "   - **No Multicollinearity:** VIFs are close to 1, indicating no multicollinearity.\n",
    "   - **With Multicollinearity:** VIFs are extremely high, indicating severe multicollinearity.\n",
    "\n",
    "3. **Linear Regression Coefficients:**\n",
    "   - **No Multicollinearity:** Coefficients are stable and close to true values (3 for X1 and 5 for X2) with low standard errors.\n",
    "   - **With Multicollinearity:** Coefficients are unstable, may deviate from true values, and have high standard errors, making them statistically insignificant.\n",
    "\n",
    "4. **Condition Number:**\n",
    "   - **No Multicollinearity:** Low condition number (~1), indicating a well-conditioned matrix.\n",
    "   - **With Multicollinearity:** High condition number (>100), indicating an ill-conditioned matrix and numerical instability.\n",
    "\n",
    "5. **Bootstrapped Coefficient Distributions:**\n",
    "   - **No Multicollinearity:** Coefficient estimates are tightly clustered around the true values.\n",
    "   - **With Multicollinearity:** Coefficient estimates are widely dispersed, indicating high variability and unreliability."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
