{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b8a04afe",
   "metadata": {},
   "source": [
    "# Decision Tree Implementation from Scratch\n",
    "\n",
    "This notebook demonstrates how to implement a Decision Tree classifier from scratch. Decision Trees are versatile machine learning algorithms that can be used for both classification and regression tasks.\n",
    "\n",
    "## Theory Overview\n",
    "\n",
    "### Decision Tree Basics\n",
    "A decision tree makes predictions by:\n",
    "1. Splitting the data based on feature values\n",
    "2. Creating a tree structure of decision nodes\n",
    "3. Making predictions based on the majority class in leaf nodes\n",
    "\n",
    "### Key Concepts\n",
    "- **Information Gain**: Measures the reduction in entropy after a split\n",
    "- **Entropy**: Measures the impurity/randomness in the dataset\n",
    "- **Gini Index**: Alternative to entropy for measuring node impurity\n",
    "- **Pruning**: Technique to prevent overfitting by limiting tree growth\n",
    "\n",
    "### Advantages\n",
    "- Easy to understand and interpret\n",
    "- Can handle both numerical and categorical data\n",
    "- Requires little data preprocessing\n",
    "- Can model non-linear relationships"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b8965d1",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60534e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from graphviz import Digraph\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da784629",
   "metadata": {},
   "source": [
    "## 2. Decision Tree Node Implementation\n",
    "\n",
    "First, we'll implement the Node class that will form our tree structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "132089e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    \"\"\"\n",
    "    A Decision Tree Node\n",
    "    \n",
    "    Attributes:\n",
    "        feature_index: Index of the feature to split on\n",
    "        threshold: Value to split the feature on\n",
    "        left: Left child node\n",
    "        right: Right child node\n",
    "        value: Predicted value (for leaf nodes)\n",
    "        info_gain: Information gain from this split\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.feature_index = None\n",
    "        self.threshold = None\n",
    "        self.left = None\n",
    "        self.right = None\n",
    "        self.value = None\n",
    "        self.info_gain = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69497309",
   "metadata": {},
   "source": [
    "## 3. Decision Tree Classifier Implementation\n",
    "\n",
    "We'll implement the Decision Tree classifier with the following key methods:\n",
    "- Entropy calculation\n",
    "- Information gain computation\n",
    "- Best split finding\n",
    "- Tree building\n",
    "- Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e219b046",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTreeClassifier:\n",
    "    def __init__(self, max_depth=None, min_samples_split=2):\n",
    "        \"\"\"\n",
    "        Initialize Decision Tree Classifier\n",
    "        \n",
    "        Parameters:\n",
    "            max_depth: Maximum depth of the tree\n",
    "            min_samples_split: Minimum samples required to split a node\n",
    "        \"\"\"\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.root = None\n",
    "        \n",
    "    def entropy(self, y):\n",
    "        \"\"\"Calculate entropy of the target variable\"\"\"\n",
    "        classes = np.unique(y)\n",
    "        entropy = 0\n",
    "        for cls in classes:\n",
    "            p = len(y[y == cls]) / len(y)\n",
    "            entropy -= p * np.log2(p)\n",
    "        return entropy\n",
    "    \n",
    "    def information_gain(self, parent, left_child, right_child):\n",
    "        \"\"\"Calculate information gain from a split\"\"\"\n",
    "        weight_left = len(left_child) / len(parent)\n",
    "        weight_right = len(right_child) / len(parent)\n",
    "        gain = self.entropy(parent) - (\n",
    "            weight_left * self.entropy(left_child) +\n",
    "            weight_right * self.entropy(right_child)\n",
    "        )\n",
    "        return gain\n",
    "    \n",
    "    def best_split(self, X, y):\n",
    "        \"\"\"Find the best split for a node\"\"\"\n",
    "        best_gain = -1\n",
    "        best_split = None\n",
    "        n_samples, n_features = X.shape\n",
    "        \n",
    "        for feature_index in range(n_features):\n",
    "            feature_values = X[:, feature_index]\n",
    "            thresholds = np.unique(feature_values)\n",
    "            \n",
    "            for threshold in thresholds:\n",
    "                left_mask = feature_values <= threshold\n",
    "                right_mask = ~left_mask\n",
    "                \n",
    "                if len(y[left_mask]) == 0 or len(y[right_mask]) == 0:\n",
    "                    continue\n",
    "                \n",
    "                gain = self.information_gain(\n",
    "                    y,\n",
    "                    y[left_mask],\n",
    "                    y[right_mask]\n",
    "                )\n",
    "                \n",
    "                if gain > best_gain:\n",
    "                    best_gain = gain\n",
    "                    best_split = {\n",
    "                        'feature_index': feature_index,\n",
    "                        'threshold': threshold,\n",
    "                        'info_gain': gain\n",
    "                    }\n",
    "        \n",
    "        return best_split\n",
    "    \n",
    "    def build_tree(self, X, y, depth=0):\n",
    "        \"\"\"Recursively build the decision tree\"\"\"\n",
    "        n_samples, n_features = X.shape\n",
    "        n_classes = len(np.unique(y))\n",
    "        \n",
    "        # Check stopping criteria\n",
    "        if (self.max_depth is not None and depth >= self.max_depth) or            n_samples < self.min_samples_split or            n_classes == 1:\n",
    "            leaf_value = np.argmax(np.bincount(y))\n",
    "            leaf = Node()\n",
    "            leaf.value = leaf_value\n",
    "            return leaf\n",
    "        \n",
    "        # Find best split\n",
    "        best_split = self.best_split(X, y)\n",
    "        if best_split is None:\n",
    "            leaf_value = np.argmax(np.bincount(y))\n",
    "            leaf = Node()\n",
    "            leaf.value = leaf_value\n",
    "            return leaf\n",
    "        \n",
    "        # Create node\n",
    "        node = Node()\n",
    "        node.feature_index = best_split['feature_index']\n",
    "        node.threshold = best_split['threshold']\n",
    "        node.info_gain = best_split['info_gain']\n",
    "        \n",
    "        # Split data\n",
    "        left_mask = X[:, node.feature_index] <= node.threshold\n",
    "        right_mask = ~left_mask\n",
    "        \n",
    "        # Build children\n",
    "        node.left = self.build_tree(X[left_mask], y[left_mask], depth + 1)\n",
    "        node.right = self.build_tree(X[right_mask], y[right_mask], depth + 1)\n",
    "        \n",
    "        return node\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Train the decision tree\"\"\"\n",
    "        self.n_classes = len(np.unique(y))\n",
    "        self.n_features = X.shape[1]\n",
    "        self.root = self.build_tree(X, y)\n",
    "        \n",
    "    def predict_single(self, x, node):\n",
    "        \"\"\"Make prediction for a single sample\"\"\"\n",
    "        if node.value is not None:\n",
    "            return node.value\n",
    "        \n",
    "        if x[node.feature_index] <= node.threshold:\n",
    "            return self.predict_single(x, node.left)\n",
    "        return self.predict_single(x, node.right)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Make predictions for multiple samples\"\"\"\n",
    "        return np.array([self.predict_single(x, self.root) for x in X])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e893302",
   "metadata": {},
   "source": [
    "## 4. Generate Synthetic Dataset\n",
    "\n",
    "We'll create a simple 2D dataset for visualization purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ea427df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data(n_samples=100):\n",
    "    \"\"\"Generate synthetic classification data\"\"\"\n",
    "    X = np.random.randn(n_samples, 2)\n",
    "    y = (X[:, 0] + X[:, 1] > 0).astype(int)\n",
    "    return X, y\n",
    "\n",
    "# Generate and split data\n",
    "X, y = generate_data(n_samples=200)\n",
    "split_idx = len(X) // 2\n",
    "X_train, X_test = X[:split_idx], X[split_idx:]\n",
    "y_train, y_test = y[:split_idx], y[split_idx:]\n",
    "\n",
    "# Plot the data\n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.scatter(X_train[y_train == 0][:, 0], X_train[y_train == 0][:, 1], \n",
    "           label='Class 0', alpha=0.6)\n",
    "plt.scatter(X_train[y_train == 1][:, 0], X_train[y_train == 1][:, 1], \n",
    "           label='Class 1', alpha=0.6)\n",
    "plt.title('Training Data')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.scatter(X_test[y_test == 0][:, 0], X_test[y_test == 0][:, 1], \n",
    "           label='Class 0', alpha=0.6)\n",
    "plt.scatter(X_test[y_test == 1][:, 0], X_test[y_test == 1][:, 1], \n",
    "           label='Class 1', alpha=0.6)\n",
    "plt.title('Testing Data')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9e9bf48",
   "metadata": {},
   "source": [
    "## 5. Train and Evaluate the Model\n",
    "\n",
    "Let's train our decision tree and evaluate its performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0143425",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "dt = DecisionTreeClassifier(max_depth=3)\n",
    "dt.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_train = dt.predict(X_train)\n",
    "y_pred_test = dt.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "train_accuracy = np.mean(y_pred_train == y_train)\n",
    "test_accuracy = np.mean(y_pred_test == y_test)\n",
    "\n",
    "print(f\"Training Accuracy: {train_accuracy:.4f}\")\n",
    "print(f\"Testing Accuracy: {test_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "497d08a9",
   "metadata": {},
   "source": [
    "## 6. Decision Boundary Visualization\n",
    "\n",
    "Let's visualize the decision boundaries created by our tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7277af66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_decision_boundary(model, X, y, title):\n",
    "    \"\"\"Plot decision boundary and data points\"\"\"\n",
    "    h = 0.02  # step size in the mesh\n",
    "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                         np.arange(y_min, y_max, h))\n",
    "\n",
    "    # Make predictions on the mesh\n",
    "    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "\n",
    "    # Plot the decision boundary\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.contourf(xx, yy, Z, alpha=0.4)\n",
    "    plt.scatter(X[y == 0][:, 0], X[y == 0][:, 1], \n",
    "               label='Class 0', alpha=0.6)\n",
    "    plt.scatter(X[y == 1][:, 0], X[y == 1][:, 1], \n",
    "               label='Class 1', alpha=0.6)\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Feature 1')\n",
    "    plt.ylabel('Feature 2')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# Plot decision boundaries\n",
    "plot_decision_boundary(dt, X_train, y_train, 'Decision Tree Boundaries (Training Data)')\n",
    "plot_decision_boundary(dt, X_test, y_test, 'Decision Tree Boundaries (Testing Data)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b97c506",
   "metadata": {},
   "source": [
    "## 7. Tree Structure Visualization\n",
    "\n",
    "Finally, let's visualize the structure of our decision tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff840664",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_tree(tree, filename='decision_tree'):\n",
    "    \"\"\"Visualize the decision tree using graphviz\"\"\"\n",
    "    dot = Digraph(comment='Decision Tree')\n",
    "    \n",
    "    def add_nodes(node, node_id='0'):\n",
    "        if node.value is not None:\n",
    "            dot.node(node_id, f'Class: {node.value}')\n",
    "            return\n",
    "        \n",
    "        dot.node(node_id, f'X[{node.feature_index}] <= {node.threshold:.2f}\\nGain: {node.info_gain:.4f}')\n",
    "        \n",
    "        # Add left child\n",
    "        left_id = f'{node_id}0'\n",
    "        add_nodes(node.left, left_id)\n",
    "        dot.edge(node_id, left_id, 'True')\n",
    "        \n",
    "        # Add right child\n",
    "        right_id = f'{node_id}1'\n",
    "        add_nodes(node.right, right_id)\n",
    "        dot.edge(node_id, right_id, 'False')\n",
    "    \n",
    "    add_nodes(tree.root)\n",
    "    dot.render(filename, view=True, format='png')\n",
    "\n",
    "# Visualize the tree\n",
    "visualize_tree(dt)"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
