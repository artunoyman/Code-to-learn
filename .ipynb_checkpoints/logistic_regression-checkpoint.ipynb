{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e2b48b9",
   "metadata": {},
   "source": [
    "# Logistic Regression Implementation from Scratch\n",
    "\n",
    "This notebook demonstrates the implementation of Logistic Regression for binary classification using:\n",
    "- Custom implementation of gradient descent optimization\n",
    "- Visualization of decision boundaries\n",
    "- Cost function monitoring\n",
    "\n",
    "Originally created by hilmi, adapted to Jupyter notebook format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4644c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32bdc68f",
   "metadata": {},
   "source": [
    "## 1. Generating the Dataset\n",
    "We'll create a synthetic dataset with two classes, each following a different multivariate normal distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c27e3ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of samples per class\n",
    "n_samples = 100\n",
    "\n",
    "# Generate features for class 0\n",
    "mean0 = [2, 2]\n",
    "cov0 = [[1, 0.5], [0.5, 1]]\n",
    "X0 = np.random.multivariate_normal(mean0, cov0, n_samples)\n",
    "y0 = np.zeros(n_samples)\n",
    "\n",
    "# Generate features for class 1\n",
    "mean1 = [4, 4]\n",
    "cov1 = [[1, 0.5], [0.5, 1]]\n",
    "X1 = np.random.multivariate_normal(mean1, cov1, n_samples)\n",
    "y1 = np.ones(n_samples)\n",
    "\n",
    "# Combine the data\n",
    "X = np.vstack((X0, X1))\n",
    "y = np.hstack((y0, y1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dbfa01d",
   "metadata": {},
   "source": [
    "## 2. Visualizing the Data\n",
    "Let's plot our synthetic dataset to see the distribution of the two classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "810640c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,6))\n",
    "plt.scatter(X0[:, 0], X0[:, 1], color='blue', label='Class 0')\n",
    "plt.scatter(X1[:, 0], X1[:, 1], color='red', label='Class 1')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.title('Synthetic Dataset for Logistic Regression')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d282035a",
   "metadata": {},
   "source": [
    "## 3. Implementing Logistic Regression\n",
    "We'll implement three key components:\n",
    "1. Sigmoid function for probability estimation\n",
    "2. Cost function for binary cross-entropy\n",
    "3. Gradient descent optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfe41179",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    \"\"\"Sigmoid activation function\"\"\"\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def compute_cost(X, y, theta):\n",
    "    \"\"\"\n",
    "    Compute binary cross-entropy loss\n",
    "    \n",
    "    Parameters:\n",
    "    X: Feature matrix with intercept term\n",
    "    y: Target labels (0 or 1)\n",
    "    theta: Model parameters\n",
    "    \"\"\"\n",
    "    m = len(y)\n",
    "    z = np.dot(X, theta)\n",
    "    h = sigmoid(z)\n",
    "    # To avoid log(0), add a small epsilon inside the log\n",
    "    epsilon = 1e-5\n",
    "    cost = -(1/m) * np.sum(y*np.log(h + epsilon) + (1 - y)*np.log(1 - h + epsilon))\n",
    "    return cost\n",
    "\n",
    "def gradient_descent(X, y, theta, alpha, num_iters):\n",
    "    \"\"\"\n",
    "    Perform gradient descent optimization\n",
    "    \n",
    "    Parameters:\n",
    "    X: Feature matrix with intercept term\n",
    "    y: Target labels (0 or 1)\n",
    "    theta: Initial model parameters\n",
    "    alpha: Learning rate\n",
    "    num_iters: Number of iterations\n",
    "    \n",
    "    Returns:\n",
    "    theta: Optimized model parameters\n",
    "    cost_history: List of costs at each iteration\n",
    "    \"\"\"\n",
    "    m = len(y)\n",
    "    cost_history = []\n",
    "    \n",
    "    for i in range(num_iters):\n",
    "        z = np.dot(X, theta)\n",
    "        h = sigmoid(z)\n",
    "        gradient = np.dot(X.T, (h - y)) / m\n",
    "        theta -= alpha * gradient\n",
    "        cost = compute_cost(X, y, theta)\n",
    "        cost_history.append(cost)\n",
    "        \n",
    "        if i % 1000 == 0:\n",
    "            print(f'Cost at iteration {i}: {cost}')\n",
    "    \n",
    "    return theta, cost_history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdfa8dfc",
   "metadata": {},
   "source": [
    "## 4. Training the Model\n",
    "Now we'll prepare the data and train our logistic regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da53454b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add intercept term to X\n",
    "X_intercept = np.hstack((np.ones((X.shape[0], 1)), X))\n",
    "\n",
    "# Initialize theta parameters to zeros\n",
    "theta_initial = np.zeros(X_intercept.shape[1])\n",
    "\n",
    "# Set hyperparameters\n",
    "alpha = 0.1        # Learning rate\n",
    "num_iters = 10000  # Number of iterations\n",
    "\n",
    "# Train the model\n",
    "theta_final, cost_history = gradient_descent(X_intercept, y, theta_initial, alpha, num_iters)\n",
    "\n",
    "print(f'\\nFinal parameters: {theta_final}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dfec72c",
   "metadata": {},
   "source": [
    "## 5. Visualizing the Decision Boundary\n",
    "We'll create a function to plot the decision boundary along with the data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b37888",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_decision_boundary(X, y, theta):\n",
    "    plt.figure(figsize=(8,6))\n",
    "    \n",
    "    # Plot the data points\n",
    "    plt.scatter(X[y==0][:,1], X[y==0][:,2], color='blue', label='Class 0')\n",
    "    plt.scatter(X[y==1][:,1], X[y==1][:,2], color='red', label='Class 1')\n",
    "    \n",
    "    # Plot the decision boundary\n",
    "    x_values = [np.min(X[:,1]-1), np.max(X[:,1]+1)]\n",
    "    y_values = - (theta[0] + np.dot(theta[1], x_values)) / theta[2]\n",
    "    plt.plot(x_values, y_values, label='Decision Boundary')\n",
    "    \n",
    "    plt.xlabel('Feature 1')\n",
    "    plt.ylabel('Feature 2')\n",
    "    plt.title('Logistic Regression Decision Boundary')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "plot_decision_boundary(X_intercept, y, theta_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "582b2185",
   "metadata": {},
   "source": [
    "## 6. Plotting Cost Function Over Iterations\n",
    "Finally, let's visualize how the cost function decreased during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31ad5434",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,6))\n",
    "plt.plot(range(num_iters), cost_history, color='purple')\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Cost')\n",
    "plt.title('Cost Function Over Iterations')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
